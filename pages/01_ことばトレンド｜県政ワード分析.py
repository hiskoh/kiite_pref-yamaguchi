import os
import io
import re 
import json
import urllib.request
from collections import Counter, defaultdict
import pandas as pd

import streamlit as st
import boto3
import zstandard as zstd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

import itertools
import networkx as nx
from pyvis.network import Network
import streamlit.components.v1 as components
from networkx.algorithms.community import greedy_modularity_communities

import json as _json
import math


st.set_page_config(page_title="ã“ã¨ã°ãƒˆãƒ¬ãƒ³ãƒ‰çœŒæ”¿", layout="wide", page_icon="âš–ï¸")

# ========================
# è¨­å®š
# ========================
# S3ã®å ´æ‰€
S3_BUCKET =  st.secrets["AWS-KEY"]["DATA_BUCKET_NAME"]
S3_KEY    = "pref_yamaguchi/trending-words/pref-yamaguchi.jsonl.zst"
AWS_REGION = "us-west-2"  
AWS_ACCESS_KEY = st.secrets["AWS-KEY"]["AWS_ACCESS_KEY"]
AWS_SECRET_KEY = st.secrets["AWS-KEY"]["AWS_SECRET_KEY"]

def make_s3_client():
    return boto3.client(
        "s3",
        aws_access_key_id=AWS_ACCESS_KEY,
        aws_secret_access_key=AWS_SECRET_KEY,
        region_name=AWS_REGION,
    )

# ========================
# ãƒ•ã‚©ãƒ³ãƒˆï¼ˆæ—¥æœ¬èªè¡¨ç¤ºç”¨ï¼‰
# ========================
def get_font_path():
    font_dir = "./font"
    os.makedirs(font_dir, exist_ok=True)
    font_path = os.path.join(font_dir, "NotoSansCJKjp-Regular.otf")
    if not os.path.exists(font_path):
        url = "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf"
        urllib.request.urlretrieve(url, font_path)
    return font_path

# ========================
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆS3ã®JSONL.zstï¼‰
# ========================
@st.cache_data(ttl=1800, show_spinner=False)  # 30åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def load_preagg_records(bucket: str, key: str):
    s3 = make_s3_client()
    obj = s3.get_object(Bucket=bucket, Key=key)
    body = obj["Body"]  # StreamingBody

    dctx = zstd.ZstdDecompressor()
    records = []

    with dctx.stream_reader(body) as reader:
        text_stream = io.TextIOWrapper(reader, encoding="utf-8", errors="strict", newline="")
        for line in text_stream:
            line = line.strip()
            if not line:
                continue
            try:
                records.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    return records

# ========================
# ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æç”»
# ========================
def draw_wordcloud(freq: dict):
    if not freq:
        st.info("å¯¾è±¡æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹èªå¥ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    font_path = get_font_path()
    wc = WordCloud(
        font_path=font_path,
        width=800,
        height=400,
        background_color="white",
        colormap="tab10",
    )
    wc.generate_from_frequencies(freq)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    st.pyplot(fig)

# ========================
# åˆç®—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ========================
def aggregate_terms(selected_records):
    counter = Counter()
    total_utter = 0
    for r in selected_records:
        total_utter += int(r.get("utterances", 0))
        for term, cnt in r.get("top_terms", []):
            counter[term] += int(cnt)
    return counter, total_utter

# ========================
# å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰
# ========================
@st.cache_data(ttl=1800, show_spinner=False)
def build_cooccurrence(records, global_freq: Counter, *,
                       top_k_per_doc: int = 30,
                       max_nodes_global: int = 80,
                       weight_mode: str = "binary"  # "binary" or "mincnt"
                       ):
    allowed_terms = set([t for t, _ in global_freq.most_common(max_nodes_global)])

    edge_weight = Counter()
    node_weight = Counter()

    for r in records:
        terms = r.get("top_terms", [])[:top_k_per_doc]
        terms = [(t, int(c)) for t, c in terms if t in allowed_terms]

        for t, c in terms:
            node_weight[t] += c

        for (t1, c1), (t2, c2) in itertools.combinations(terms, 2):
            a, b = sorted((t1, t2))
            if weight_mode == "mincnt":
                edge_weight[(a, b)] += min(c1, c2)
            else:
                edge_weight[(a, b)] += 1

    G = nx.Graph()
    for t, w in node_weight.items():
        G.add_node(t, size=w)
    for (a, b), w in edge_weight.items():
        G.add_edge(a, b, weight=w)
    return G

# ========================
# å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’pyvisã§å¯è¦–åŒ–
# ========================
def render_pyvis_network(
    G: nx.Graph,
    *,
    min_edge_weight: int = 2,
    physics: bool = False,
    height_px: int = 720,
    label_font_size: int = 22,
    enable_clustering: bool = True,
    focus_community: int | None = None,
    drop_isolates: bool = True,             
    keep_largest_component: bool = False    
):
    H = nx.Graph()
    for n, attrs in G.nodes(data=True):
        H.add_node(n, **attrs)
    for u, v, attrs in G.edges(data=True):
        if int(attrs.get("weight", 1)) >= min_edge_weight:
            H.add_edge(u, v, **attrs)

    if drop_isolates:
        H.remove_nodes_from(list(nx.isolates(H)))

    if keep_largest_component and H.number_of_nodes() > 0:
        comps = list(nx.connected_components(H))
        giant = max(comps, key=len)
        H = H.subgraph(giant).copy()

    comm_map, comms = ({}, [])
    if enable_clustering and H.number_of_nodes() > 0:
        comm_map, comms = detect_communities(H)

    if focus_community is not None and enable_clustering and comms:
        keep = comms[focus_community] if focus_community < len(comms) else set()
        H = H.subgraph(keep).copy()
        if H.number_of_nodes() > 0:
            comm_map, comms = detect_communities(H)

    pos = layout_communities_with_warmstart(
        H, comms,
        cluster_k=2.0,
        local_k=0.5,
        cluster_scale=900,
        local_scale=550,
        seed=42
    )

    net = Network(
        height=f"{height_px}px",
        width="100%",
        bgcolor="#ffffff",
        font_color="#1f2937",
        notebook=False,
        directed=False,
    )

    for n, attrs in H.nodes(data=True):
        val = int(attrs.get("size", 1))
        group = comm_map.get(n) if enable_clustering else None
        x, y = pos.get(n, (None, None))
        net.add_node(
            n,
            label=n,
            value=val,
            title=f"{n}: {val}",
            group=group,
            x=x, y=y,
            physics=physics,
            fixed=not physics
        )

    for u, v, attrs in H.edges(data=True):
        w = int(attrs.get("weight", 1))
        net.add_edge(u, v, value=w, title=f"co-occur: {w}")

    options = {
        "nodes": {
            "font": {"size": label_font_size, "face": "sans-serif"},
            "scaling": {
                "min": 10, "max": 50,
                "label": {"enabled": True,
                          "min": max(10, label_font_size-6),
                          "max": label_font_size+10}
            }
        },
        "edges": {"smooth": False},
        "interaction": {"tooltipDelay": 100, "hover": True},
        "physics": {
            "enabled": physics,
            "solver": "forceAtlas2Based",
            "forceAtlas2Based": {
                "gravitationalConstant": -50,
                "springLength": 120,
                "springConstant": 0.08,
                "avoidOverlap": 0.6,
                "damping": 0.45
            },
            "stabilization": {"enabled": physics, "iterations": 300},
            "minVelocity": 0.5,
            "maxVelocity": 30
        }
    }
    net.set_options(_json.dumps(options))
    html = net.generate_html(notebook=False)
    components.html(html, height=height_px, scrolling=True)
    return comms

def community_meta_graph(H: nx.Graph, comms):
    M = nx.Graph()
    for i, nodes in enumerate(comms):
        M.add_node(i, size=len(nodes))
    for u, v, d in H.edges(data=True):
        cu = next(i for i, S in enumerate(comms) if u in S)
        cv = next(i for i, S in enumerate(comms) if v in S)
        if cu == cv:
            continue
        w = int(d.get("weight", 1))
        if M.has_edge(cu, cv):
            M[cu][cv]["weight"] += w
        else:
            M.add_edge(cu, cv, weight=w)
    return M

def layout_communities_with_warmstart(
    H: nx.Graph,
    comms,
    *,
    cluster_k: float = 2.0,
    local_k: float = 0.5,
    cluster_scale: float = 900,
    local_scale: float = 550,
    seed: int = 42
):
    if not comms:
        return nx.spring_layout(H, k=local_k, weight="weight", seed=seed)

    M = community_meta_graph(H, comms)
    pos_comm = nx.spring_layout(M, k=cluster_k, weight="weight", seed=seed)

    pos = {}
    for i, nodes in enumerate(comms):
        sub = H.subgraph(nodes)
        local = nx.spring_layout(sub, k=local_k, weight="weight", seed=seed)
        cx, cy = pos_comm.get(i, (0.0, 0.0))
        cx *= cluster_scale
        cy *= cluster_scale
        for n, (x, y) in local.items():
            pos[n] = (cx + x * local_scale, cy + y * local_scale)
    return pos

def detect_communities(G: nx.Graph):
    if G.number_of_nodes() == 0:
        return {}, []
    comms = list(greedy_modularity_communities(G, weight="weight"))
    comm_map = {}
    for i, s in enumerate(comms):
        for n in s:
            comm_map[n] = i
    return comm_map, comms

def summarize_communities(G: nx.Graph, comms):
    summaries = []
    for i, nodes in enumerate(comms):
        rows = sorted(
            [(n, int(G.nodes[n].get("size", 1))) for n in nodes],
            key=lambda x: x[1],
            reverse=True
        )[:10]
        summaries.append({"community": i, "top_terms": rows, "size": len(nodes)})
    return summaries

def layout_by_community(H: nx.Graph, comms, *, intra_k=0.5, spacing=800, seed=42):
    if not comms:
        return nx.spring_layout(H, k=intra_k, weight="weight", seed=seed)

    pos = {}
    n_comm = len(comms)
    for i, nodes in enumerate(comms):
        angle = 2 * math.pi * i / n_comm
        cx = spacing * math.cos(angle)
        cy = spacing * math.sin(angle)
        sub = H.subgraph(nodes)
        local = nx.spring_layout(sub, k=intra_k, weight="weight", seed=seed)
        for n, (x, y) in local.items():
            pos[n] = (x * 300 + cx, y * 300 + cy)
    return pos

def layout_by_community_grid(H: nx.Graph, comms, *,
                             cluster_spacing=1200,
                             subgraph_scale=500,
                             grid_cols=3,
                             seed=42):
    if not comms:
        return nx.spring_layout(H, k=0.5, weight="weight", seed=seed)
    pos = {}
    for i, nodes in enumerate(comms):
        sub = H.subgraph(nodes)
        local = nx.spring_layout(sub, k=0.5, weight="weight", seed=seed)
        row, col = divmod(i, grid_cols)
        cx = col * cluster_spacing
        cy = row * cluster_spacing
        for n, (x, y) in local.items():
            pos[n] = (x * subgraph_scale + cx, y * subgraph_scale + cy)
    return pos

def _edge_count_at_threshold(G: nx.Graph, thr: int) -> int:
    return sum(1 for _, _, d in G.edges(data=True) if int(d.get("weight", 1)) >= thr)

def recommend_min_edge(G: nx.Graph, start_thr: int, *, target_max_edges: int, cap: int,
                       step_back_if_below: bool = True) -> tuple[int, int]:
    thr = start_thr
    edges = _edge_count_at_threshold(G, thr)
    while edges > target_max_edges and thr < cap:
        thr += 1
        edges = _edge_count_at_threshold(G, thr)
    if thr > start_thr and edges < target_max_edges * 0.6:
        prev_edges = _edge_count_at_threshold(G, thr - 1)
        return thr - 1, prev_edges
    return thr, edges

def _mark_min_edge_touched():
    st.session_state["min_edge_user_touched"] = True

def parse_date_from_chunk_head(chunk_id: str) -> pd.Timestamp | None:
    if not chunk_id:
        return None
    head = str(chunk_id).split("_", 1)[0]
    m = re.match(r"^\s*(\d{4})å¹´\s*(\d{1,2})æœˆ\s*(\d{1,2})æ—¥", head)
    if not m:
        return None
    y, mo, d = map(int, m.groups())
    try:
        return pd.Timestamp(y, mo, d)
    except Exception:
        return None

# ========================
# UI æœ¬ä½“
# ========================
st.title("âš–ï¸ã“ã¨ã°ãƒˆãƒ¬ãƒ³ãƒ‰ï½œçœŒæ”¿ãƒ¯ãƒ¼ãƒ‰åˆ†æ")
st.markdown("""
<span style="color:#6b7280">
ç™ºè¨€è€…ã‚„ä¼šè­°ãªã©ã‚’é¸ã‚“ã§ã€ç™ºè¨€ã®å‚¾å‘ã‚’ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚<br>
å¹´åº¦ã‚„ä¼šè­°ã€ç™ºè¨€è€…ã‚’å¤‰ãˆã¦ã€ç™ºè¨€ã®å‚¾å‘ã‚„ç‰¹å¾´ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
</span>
""", unsafe_allow_html=True)

st.markdown("""
<style>
.stMultiSelect [data-baseweb="tag"]{
  background-color:#eef3f8 !important;
  color:#1f2937 !important;
  border:1px solid #d1d5db !important;
}
.stMultiSelect [data-baseweb="tag"] [data-baseweb="tag-close-icon"]{
  color:#6b7280 !important;
}
</style>
""", unsafe_allow_html=True)

with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™â€¦"):
    try:
        data = load_preagg_records(S3_BUCKET, S3_KEY)
    except Exception as e:
        st.error("ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã®èª­è¾¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        st.stop()

if not data:
    st.warning("äº‹å‰é›†è¨ˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã—ãŸã€‚")
    st.stop()

# --------- ã“ã“ã§ session_state ã‚’åˆæœŸåŒ–ï¼ˆNameError/KeyErrorå›é¿ï¼‰---------
DEFAULT_TOPK = 30
DEFAULT_MAXN = 80
DEFAULT_MINEDGE = 2
DEFAULT_LABEL_FONT = 22
DEFAULT_WEIGHT_MODE = "binary"
DEFAULT_PHYSICS = True

ss = st.session_state
ss.setdefault("top_k_per_doc", DEFAULT_TOPK)
ss.setdefault("max_nodes_global", DEFAULT_MAXN)
ss.setdefault("min_edge_weight", DEFAULT_MINEDGE)
ss.setdefault("weight_mode", DEFAULT_WEIGHT_MODE)
ss.setdefault("label_font_size", DEFAULT_LABEL_FONT)
ss.setdefault("physics_on", DEFAULT_PHYSICS)
ss.setdefault("min_edge_user_touched", False)
ss.setdefault("auto_min_edge", True)
ss.setdefault("target_max_edges", 200)
ss.setdefault("min_edge_cap", 20)

# ========= å½¹è·ã®é™å®šï¼ˆçŸ¥äº‹ãƒ»å§”å“¡ã®ã¿ï¼‰ + ä¼šè­°åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹/ç¨®åˆ¥ã‚’ä»˜ä¸ =========
ALLOWED_ROLES = {"çŸ¥äº‹", "å§”å“¡"}

def get_meeting_prefix(name: str | None) -> str | None:
    if not name:
        return None
    # å…¨è§’ãƒã‚¤ãƒ•ãƒ³ã§åˆ†å‰²ã€æ‰‹å‰ã‚’æ¡ç”¨
    return str(name).split("ï¼")[0].strip()

def classify_meeting_type(name: str | None) -> str | None:
    """'å§”å“¡ä¼š' ã‚’å«ã‚€ã‹ã©ã†ã‹ã§åˆ†é¡ã€‚å«ã‚ã° 'å§”å“¡ä¼š'ã€ãã‚Œä»¥å¤–ã¯ 'å®šä¾‹ä¼šãƒ»è‡¨æ™‚ä¼š'ã€‚"""
    if not name:
        return None
    s = str(name)
    return "å§”å“¡ä¼š" if "å§”å“¡ä¼š" in s else "å®šä¾‹ä¼šãƒ»è‡¨æ™‚ä¼š"

for r in data:
    r["_meeting_prefix"] = get_meeting_prefix(r.get("meeting_name"))
    r["_meeting_type"]   = classify_meeting_type(r.get("meeting_name"))

# ========= ãƒ•ã‚£ãƒ«ã‚¿ç”¨ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ï¼ˆçŸ¥äº‹ãƒ»å§”å“¡ã®ã¿ã‚’å¯¾è±¡ï¼‰=========
years = sorted({
    r.get("year")
    for r in data
    if r.get("speaker_role") in ALLOWED_ROLES and r.get("year")
})
speakers = sorted({
    r.get("speaker")
    for r in data
    if r.get("speaker_role") in ALLOWED_ROLES and r.get("speaker")
})
meeting_types = [t for t in ["å®šä¾‹ä¼šãƒ»è‡¨æ™‚ä¼š", "å§”å“¡ä¼š"]
                 if any(r.get("speaker_role") in ALLOWED_ROLES and r.get("_meeting_type")==t for r in data)]
meeting_prefixes_all = sorted({
    r.get("_meeting_prefix")
    for r in data
    if r.get("speaker_role") in ALLOWED_ROLES and r.get("_meeting_prefix")
})

# ========= å½¹è·ã‚»ãƒ¬ã‚¯ã‚¿ã¯éè¡¨ç¤ºã€‚1è¡Œã§ï¼ˆç™ºè¨€è€…ï¼å¹´ï¼ä¼šè­°ç¨®åˆ¥ï¼ä¼šè­°åï¼‰ =========
st.divider()
st.subheader("æ¡ä»¶")

def expand_all(selected, universe):
    if ("ã™ã¹ã¦" in selected) or (not selected):
        return set(universe)
    return set(selected)

c1, c2, c3, c4 = st.columns(4)

with c1:
    speaker_options = ["ã™ã¹ã¦"] + speakers
    current_speaker = st.session_state.get("speaker_sb", "ã™ã¹ã¦")
    if current_speaker not in speaker_options:
        current_speaker = "ã™ã¹ã¦"
    sel_speaker = st.selectbox("ç™ºè¨€è€…", speaker_options,
                               index=speaker_options.index(current_speaker),
                               key="speaker_sb")

with c2:
    sel_years = st.multiselect("ç™ºè¨€å¹´", ["ã™ã¹ã¦"] + years, default=["ã™ã¹ã¦"])

with c3:
    sel_meeting_types = st.multiselect("ä¼šè­°ç¨®åˆ¥", ["ã™ã¹ã¦"] + meeting_types, default=["å®šä¾‹ä¼šãƒ»è‡¨æ™‚ä¼š"])

# ä¼šè­°åï¼ˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰ã¯ã€é¸æŠã•ã‚ŒãŸä¼šè­°ç¨®åˆ¥ã«å¿œã˜ã¦å€™è£œã‚’çµã‚‹
if ("ã™ã¹ã¦" in sel_meeting_types) or (not sel_meeting_types):
    meeting_prefix_candidates = meeting_prefixes_all
else:
    selected_types = set(sel_meeting_types)
    meeting_prefix_candidates = sorted({
        r.get("_meeting_prefix")
        for r in data
        if (r.get("speaker_role") in ALLOWED_ROLES)
        and (r.get("_meeting_type") in selected_types)
        and r.get("_meeting_prefix")
    })

with c4:
    sel_meet_prefix = st.multiselect("ä¼šè­°å",
                                     ["ã™ã¹ã¦"] + meeting_prefix_candidates,
                                     default=["ã™ã¹ã¦"])

years_set = expand_all(sel_years, years)
meeting_types_set = expand_all(sel_meeting_types, meeting_types)
meet_prefix_set = expand_all(sel_meet_prefix, meeting_prefix_candidates)

# ç™ºè¨€è€…ã¯å˜ä¸€é¸æŠï¼ˆâ€œã™ã¹ã¦â€ ã¯å…¨å“¡è¨±å®¹ï¼‰
speaker_set = set(speakers) if sel_speaker == "ã™ã¹ã¦" else {sel_speaker}

# ========= å®Ÿãƒ•ã‚£ãƒ«ã‚¿ï¼ˆå½¹è·é™å®š + å¹´ + ç¨®åˆ¥ + ä¼šè­°åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ + ç™ºè¨€è€…ï¼‰=========
filtered = [
    r for r in data
    if (r.get("speaker_role") in ALLOWED_ROLES)
    and (r.get("year") in years_set)
    and (r.get("_meeting_type") in meeting_types_set)
    and (r.get("_meeting_prefix") in meet_prefix_set)
    and (r.get("speaker") in speaker_set)
]

# ===== â–¼ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ ç”»åƒè¡¨ç¤ºéƒ¨ =====
st.divider()
st.subheader("é »å‡ºå˜èª")

freq_counter, total_utterances = aggregate_terms(filtered)
draw_wordcloud(freq_counter)

# ===== â–¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ è¡¨ç¤ºéƒ¨ =====
G = build_cooccurrence(
    filtered,
    freq_counter,
    top_k_per_doc=ss["top_k_per_doc"],
    max_nodes_global=ss["max_nodes_global"],
    weight_mode=ss["weight_mode"],
)

if (
    ss.get("auto_min_edge", True) and
    not ss["min_edge_user_touched"] and
    ss["min_edge_weight"] == DEFAULT_MINEDGE
):
    new_thr, edge_cnt = recommend_min_edge(
        G,
        start_thr=ss["min_edge_weight"],
        target_max_edges=ss.get("target_max_edges", 200),
        cap=ss.get("min_edge_cap", 20),
    )
    if new_thr != ss["min_edge_weight"]:
        ss["min_edge_weight"] = new_thr
    st.caption(f"ç¾åœ¨ã®é–¾å€¤: {ss['min_edge_weight']}ï¼ˆã‚¨ãƒƒã‚¸æ•°: {edge_cnt}ï¼‰")
else:
    st.caption(f"ç¾åœ¨ã®é–¾å€¤: {ss['min_edge_weight']}ï¼ˆã‚¨ãƒƒã‚¸æ•°: {_edge_count_at_threshold(G, ss['min_edge_weight'])}ï¼‰")

st.subheader("å˜èªé–“ã®é–¢ä¿‚æ€§")

comms = render_pyvis_network(
    G,
    min_edge_weight=ss["min_edge_weight"],
    physics=ss["physics_on"],
    height_px=720,
    label_font_size=ss["label_font_size"],
    enable_clustering=True,
    focus_community=None
)

with st.expander("âš™ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=False):
    c1, c2, c3, c4, c5 = st.columns(5)
    with c1:
        st.slider("å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã§ä½¿ã†ä¸Šä½èªæ•°", 10, 100, ss["top_k_per_doc"], 5, key="top_k_per_doc")
    with c2:
        st.slider("å…¨ä½“é »åº¦ã®ä¸Šä½Nèªã¾ã§", 30, 200, ss["max_nodes_global"], 10, key="max_nodes_global")
    with c3:
        st.slider("ã‚¨ãƒƒã‚¸ã®æœ€å°å…±èµ·å›æ•°", 1, 20, ss["min_edge_weight"], 1,
                  key="min_edge_weight", on_change=_mark_min_edge_touched)
    with c4:
        st.selectbox("é‡ã¿ã®å®šç¾©", ["binary", "mincnt"],
                     index=0 if ss["weight_mode"] == "binary" else 1, key="weight_mode")
    with c5:
        st.slider("ãƒ©ãƒ™ãƒ«æ–‡å­—ã‚µã‚¤ã‚º", 12, 36, ss["label_font_size"], 1, key="label_font_size")

    st.toggle("ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹åŒ–", value=ss["physics_on"], key="physics_on")
    st.caption("â€» ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¤‰æ›´ã§è‡ªå‹•çš„ã«å†æç”»ã•ã‚Œã¾ã™ã€‚")

# ===== â–¼TOP10ï¼ˆå„èªã®æ™‚ç³»åˆ— + ä¸€ç·’ã«å‡ºã¦ããŸãƒ¯ãƒ¼ãƒ‰ï¼‰ =====
st.subheader("é »å‡ºå˜èª TOP10")
with st.expander("å‡ºç¾é »åº¦ã®é«˜ã„å˜èª TOP10", expanded=False):
    TOP_N = 10
    RELATED_K = 5
    threshold = ss["min_edge_weight"]

    top_terms = freq_counter.most_common(TOP_N)
    if not top_terms:
        st.write("ãªã—")
    else:
        for rank, (term, cnt) in enumerate(top_terms, start=1):
            cooccur_rows = []
            if G.has_node(term):
                neighbors = []
                for nbr in G.neighbors(term):
                    w = int(G[term][nbr].get("weight", 1))
                    if w >= threshold:
                        neighbors.append((nbr, w))
                neighbors.sort(key=lambda x: x[1], reverse=True)
                cooccur_rows = neighbors[:RELATED_K]

            ts_rows = []
            for r in filtered:
                local = dict((t, int(c)) for t, c in r.get("top_terms", []))
                c_term = int(local.get(term, 0))
                if c_term <= 0:
                    continue
                dt = parse_date_from_chunk_head(r.get("chunk_id", ""))
                if dt is None:
                    continue
                ts_rows.append({"x": dt, "count": c_term})

            if ts_rows:
                df_ts = pd.DataFrame(ts_rows)
                ts_series = (
                    df_ts.groupby("x", dropna=True)["count"].sum().sort_index()
                )
                df_plot = ts_series.reset_index().rename(
                    columns={"x": "æ™‚ç‚¹", "count": "å‡ºç¾æ•°"}
                )
            else:
                df_plot = pd.DataFrame(columns=["æ™‚ç‚¹", "å‡ºç¾æ•°"])

            with st.expander(f"{rank}ä½ï¼š{term}ï¼ˆé »åº¦ï¼š{cnt}å›ï¼‰", expanded=False):
                st.markdown("**è©²å½“èªãŒå‡ºç¾ã—ãŸè­°ä¼šã®æ™‚ç³»åˆ—**")
                if not df_plot.empty:
                    df_m = df_plot.copy()
                    df_m["YYYY_MM"] = (
                        df_m["æ™‚ç‚¹"]
                        .dt.to_period("M")
                        .astype(str)
                        .str.replace("-", ".", regex=False)
                    )
                    df_m = df_m.groupby("YYYY_MM", as_index=False)["å‡ºç¾æ•°"].sum()
                    st.line_chart(df_m.set_index("YYYY_MM")["å‡ºç¾æ•°"])
                else:
                    st.write("æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãªã—")

                st.markdown("**ä¸€ç·’ã«ä½¿ã‚ã‚Œã‚„ã™ã„å˜èªï¼ˆä¸Šä½5ä»¶ï¼‰**")
                if cooccur_rows:
                    st.dataframe(
                        {
                            "å˜èª": [n for n, _ in cooccur_rows],
                            "é »åº¦": [w for _, w in cooccur_rows],
                        },
                        use_container_width=True,
                        hide_index=True,
                    )
                else:
                    st.write("ãªã—ï¼ˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯¾è±¡å¤– or ã—ãã„å€¤ãŒé«˜ã„ï¼‰")

# ===== â–¼ãƒ•ãƒƒã‚¿ãƒ¼ =====
st.divider()
st.caption("""
âš ï¸ æœ¬ãƒšãƒ¼ã‚¸ã§ã¯è­°ä¼šè­°äº‹éŒ²ãªã©ã‚’å½¢æ…‹ç´ è§£æã—ã€ä¸€èˆ¬çš„ãªèªå¥ã‚’é™¤å¤–ã—ãŸã†ãˆã§é »å‡ºåè©ã‚’æŠ½å‡ºã—ã¦åˆ†æã—ã¦ã„ã¾ã™ã€‚  
âš ï¸ å‡¦ç†è»½æ¸›ã®ãŸã‚ã€å„ç™ºè¨€ã”ã¨ã«ã€Œãã®ç™ºè¨€å†…ã§é »å‡ºã—ãŸä¸Šä½100èªã€ã®ã¿ã‚’é›†è¨ˆå¯¾è±¡ã¨ã—ã¦ã„ã¾ã™ã€‚  
ğŸ™Œ æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å€‹äººã«ã‚ˆã‚Šé‹å–¶ã•ã‚Œã¦ã„ã¾ã™ã€‚ã”æ”¯æ´ã„ãŸã ã‘ã‚‹æ–¹ã¯ãœã²ã“ã¡ã‚‰ã‹ã‚‰ï¼š  
[ğŸ’› codocã§æ”¯æ´ã™ã‚‹](https://codoc.jp/sites/p8cEFlTZQA/entries/MMZnODc1dw)
""")
